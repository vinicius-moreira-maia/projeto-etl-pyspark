# T칤tulo

## 游닄 칈ndice

- [Arquitetura](#arquitetura)
- [Descri칞칚o](#descri칞칚o)
- [Ferramentas Utilizadas](#ferramentas-utilizadas)
- [Sobre o Projeto](#sobre-o-projeto)
  - [1. Arquivo start_cluster.sh](#1-arquivo-startclustersh)
  - [2. Arquivo stop_cluster.sh](#2-arquivo-stopclustersh)
- [Alguns pontos de melhoria](#alguns-pontos-de-melhoria)

## Arquitetura

![PySpark ETL](imgs/arch.png)

## Descri칞칚o

## Ferramentas Utilizadas

**1. Docker** 

**2. Postgres**

**3. PgAdmin** 

**4. PySpark**

**5. Shell Scripts**

## Sobre o Projeto

Antes de mais nada, foi necess치rio instalar tanto o java como o spark e configurar as vari치veis de ambiente para os paths necess치rios. No caso desta configura칞칚o foi utilizado o arquivo .bashrc, que configura a inicializa칞칚o do bash para o meu usu치rio, para setar as vari치veis de forma autom치tica.

![Env Vars](imgs/envvars.jpg)

### **1. Arquivo start_cluster.sh**

### **2. Arquivo stop_cluster.sh**

## Alguns pontos de melhoria:

**1. Modelagem Dimensional** 

**2. Tratamento de Dados** 